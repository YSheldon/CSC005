title:CrowdStrike Scores Highest Overall for Use Case Type A or “Forward Leaning” Organizations in Gartner’s Critical Capabilities for Endpoint Protection Platforms
date:2018-06-01 12:00:27
tourl:https://www.crowdstrike.com/blog/crowdstrike-scores-highest-overall-for-use-case-type-a-or-forward-leaning-organizations-in-gartners-critical-capabilities-for-endpoint-protection-platforms/
tags:[attack,Crowdstrike,AWS,law,Commission on Enhancing National Cybersecurity,act]
    The trouble with dwell time Dwell time, the period between when an attack occurs and when…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Event Stream Processing (ESP) has been a central component of CrowdStrike Falcon’s IOA approach since CrowdStrike's…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t For as long as we have had a cybersecurity industry, the market’s attention has been solidly…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     Users have long needed to access important resources such as virtual private networks (VPNs), web applications, and mail servers from anywhere in the world at any time. While the ability to access resources from anywhere is imperative for employees, threat actors often leverage stolen credentials to access systems and data. Due to large volumes of remote access connections, it can be difficult to distinguish between a legitimate and a malicious login. Today, we are releasing Once remote authentication activity is baselined across an environment, analysts can begin to identify authentication activity that deviates from business requirements and normalized patterns, such as: GeoLogonalyzer can help address these and similar situations by processing authentication logs containing timestamps, usernames, and source IP addresses. GeoLogonalyzer can be For a remote authentication log that records a source IP address, it is possible to estimate the location each logon originated from using data such as For example, if a user account, Meghan, logged on from New York City, New York on 2017-11-24 at 10:00:00 UTC and then logged on from Los Angeles, California 10 hours later on 2017-11-24 at 20:00:00 UTC, that is roughly a 2,450 mile change over 10 hours. Meghan’s logon source change can be normalized to 245 miles per hour which is reasonable through commercial airline travel. If a second user account, Harry, logged on from Dallas, Texas on 2017-11-25 at 17:00:00 UTC and then logged on from Sydney, Australia two hours later on 2017-11-25 at 19:00:00 UTC, that is roughly an 8,500 mile change over two hours. Harry’s logon source change can be normalized to 4,250 miles per hour, which is likely infeasible with modern travel technology. By focusing on the changes in logon sources, analysts do not have to manually review the many times that Harry might have logged in from Dallas before and after logging on from Sydney. Attackers understand that organizations may either be blocking or looking for connections from unexpected locations. One solution for attackers is to establish a proxy on either a compromised server in another country, or even through a rented server hosted in another country by companies such as AWS, DigitalOcean, or Choopa. Fortunately, Github user GeoLogonalyzer is designed to process remote access platform logs that include a timestamp, username, and source IP. Applicable log sources include, but are not limited to: GeoLogonalyzer’s built-in YYYY-MM-DD HH:MM:SS, username, source IP, optional source hostname, optional VPN client details GeoLogonalyzer’s code comments include instructions for adding customized log format support. Due to the various VPN log formats exported from VPN server manufacturers, version 1.0 of GeoLogonalyzer does not include support for raw VPN server logs. Figure 1 represents an example input GeoLogonalyzer.exe --csv VPNLogs.csv --output GeoLogonalyzedVPNLogs.csv python GeoLogonalyzer.py --csv VPNLogs.csv --output GeoLogonalyzedVPNLogs.csv Figure 2 represents the example output In the example output from Figure 2, GeoLogonalyzer helps identify the following anomalies in the Harry account’s logon patterns: Manual analysis of the data could also reveal anomalies such as: While it may be impossible to determine if a logon pattern is malicious based on this data alone, analysts can use GeoLogonalyzer to flag and investigate potentially suspicious logon activity through other investigative methods. Any RFC1918 source IP addresses, such as 192.168.X.X and 10.X.X.X, will not have a physical location registered in the MaxMind database. By default, GeoLogonalyzer will use the coordinates (0, 0) for any reserved IP address, which may alter results. Analysts can manually edit these coordinates, if desired, by modifying the RESERVED_IP_COORDINATES constant in the Python script. Setting this constant to the coordinates of your office location may provide the most accurate results, although may not be feasible if your organization has multiple locations or other point-to-point connections. GeoLogonalyzer also accepts the parameter –skip_rfc1918, which will completely ignore any RFC1918 source IP addresses and could result in missed activity. It may also be useful to include failed logon attempts and logoff records with the log source data to see anomalies related to source information of all VPN activity. At this time, GeoLogonalyzer does not distinguish between successful logons, failed logon attempts, and logoff events. GeoLogonalyzer also does not detect overlapping logon sessions from multiple source IP addresses. Note that the use of VPN or other tunneling services may create false positives. For example, a user may access an application from their home office in Wyoming at 08:00 UTC, connect to a VPN service hosted in Georgia at 08:30 UTC, and access the application again through the VPN service at 09:00 UTC. GeoLogonalyzer would process this application access log and detect that the user account required a FAST travel rate of roughly 1,250 miles per hour which may appear malicious. Establishing a baseline of legitimate authentication patterns is recommended to understand false positives. GeoLogonalyzer relies on open source data to make cloud hosting provider determinations. These lookups are only as accurate as the available open source data. Understanding that no single analysis method is perfect, the following recommendations can help security teams prevent the abuse of remote access platforms and investigate suspected compromise. Download Christopher Schmitt, Seth Summersett, Jeff Johns, and Alexander Mulfinger.  I wanted to create a how-to blog post about creating gephi visualizations, but I realized it’d probably need to include, like, a thousand embedded screenshots. So I made a video instead.For those not keeping track, the NIST Cybersecurity Framework received its first update on April 16, 2018. If you’re already familiar with the original 2014 version, fear not. Everything you know and love about version 1.0 remains in 1.1, along with a few helpful additions and clarifications. Among the most important clarifications, one in particular jumps out: If your company thought it complied with the old Framework and intends to comply with the new one, think again. Your company hasn’t been in compliance with the Framework, and it never will be. Why?  Because NIST says so. According to NIST, although companies can comply with their own cybersecurity requirements, and they can use the Framework to determine and express those requirements, there is no such thing as complying with the Framework itself. In the words of NIST, saying otherwise is confusing. The Framework should instead be “used” and “leveraged.” Which leads us to a second important clarification, this time concerning the Framework Core.  Perhaps you know the Core by its less illustrious name: “Appendix A.” Regardless, the Core is a 20-page spreadsheet that lists five Functions (Identify, Protect, Detect, Respond, and Recover); dozens of cybersecurity categories and subcategories, including such classics as “anomalous activity is detected;” and, provides Informative References of common standards, guidelines, and practices. Practitioners tend to agree that the Core is an invaluable resource when used correctly. For NIST, proper use requires that companies view the Core as a collection of potential “outcomes” to achieve rather than a checklist of “actions” to perform. Expressed differently, the Core outlines the objectives a company may wish to pursue, while providing flexibility in terms of how, and even whether, to accomplish them. So, why are these particular clarifications worthy of mention? Simply put, because they demonstrate that NIST continues to hold firm to risk-based management principles. A company cannot merely hand the NIST Framework over to its security team and tell it to check the boxes and issue a certificate of compliance. Instead, to use NIST’s words: “The Framework focuses on using business drivers to guide cybersecurity activities and considering cybersecurity risks as part of the organization’s risk management processes.” Wait, what?  That sentence is worth a second read. Its importance lies in the fact that NIST is not encouraging companies to achieve every Core outcome. Instead, organizations are expected to consider their business requirements and material risks, and then make reasonable and informed cybersecurity decisions using the Framework to help them identify and prioritize feasible and cost-effective improvements. Which leads us to discuss a particularly important addition to version 1.1. The new Framework now includes a section titled “Self-Assessing Cybersecurity Risk with the Framework.” In fact, that’s the only entirely new section of the document. Companies are encouraged to perform internal or third-party assessments using the Framework. Leading this effort requires sufficient expertise in order to accurately inform an organization of its current cybersecurity risk profile, foster discussions that lead to an agreement on the desired or “target” profile, and drive the organization’s adoption and execution of a remediation plan to address material gaps between what the company has in place and what it needs. Of course, there are many other additions to the Framework (most prominently, a stronger focus on Yes, you read that last part right, “evolution activities.” To avoid corporate extinction in today’s data- and technology-driven landscape, a famous Jack Welch quote comes to mind: “Change before you have to.” Considering its resounding adoption not only within the United States, but in other parts of the world, as well, the best time to incorporate the Framework and its revisions into your enterprise risk management program is now. Finally, if you need help assessing your cybersecurity posture and leveraging the Framework, reach out. As the old adage goes, you don’t need to know everything. You just need to know where to find what you need when you need it.       Steven Chabinsky is global chair of the Data, Privacy, and Cybersecurity practice at White  Case LLP, an international law firm, and the cyber tactics columnist for Security magazine. He previously served as a member of the President’s Commission on Enhancing National Cybersecurity, the General Counsel and Chief Risk Officer of CrowdStrike, and Deputy Assistant Director of the FBI Cyber Division. Learn more: https://www.whitecase.com/people/steven-r-chabinsky \t\t\t   \t\t \t\t \t    \tAt the Gartner Security and Risk Management Summit 2018 in June, CrowdStrike CTO and Co-Founder Dmitri…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t In an age where information is the ultimate currency, traditional strategies focused on malware, perimeter defense,…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t A comprehensive Next-Generation Endpoint Protection strategy shouldn’t just be about reacting and responding to threats, but…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     Gartner just released its inaugural The chart below shows vendor product scores for use case Type A or “forward leaning”  organizations: The Critical Capabilities research is a continuation of the analysis conducted for the CrowdStrike believes the Critical Capabilities report is an important complement to the Gartner MQ for EPP because the former focuses solely on product capabilities, rather than on factors such as vendor market share. More specifically, the Critical Capabilities report helps organizations understand which vendors offer the solutions that will best fit their needs by segmenting the assessments into different use cases. We firmly believe the reason CrowdStrike scored so high in all use cases outlined by Gartner is because of our modular architecture, which offers customers the flexibility to choose the best solution to fit their needs. The CrowdStrike Falcon® platform unifies and delivers IT Hygiene, next-generation antivirus (NGAV), endpoint detection and response (EDR), managed threat hunting, and threat intelligence — all via a single lightweight agent. We urge organizations to review both reports —  the For a free trial of CrowdStrike Falcon Prevent™ next-gen AV, clickThe Health Insurance Portability and Accountability Act (HIPAA) has big consequences for organizations of all sizes.…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Global statistics in the most recent Ponemon report on the cost of a data breach show…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t A recent interview with CrowdStrike VP of Product Marketing Dan Larson, for the CyberWire Daily Podcast,…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     