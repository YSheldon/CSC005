title:A Totally Tubular Treatise on TRITON and TriStation
date:2018-06-07 17:00:51
tourl:/blog/threat-research/2018/06/totally-tubular-treatise-on-triton-and-tristation.html
tags:[attack,Crowdstrike,AWS,law,Commission on Enhancing National Cybersecurity,act,FIREEYE]
    The trouble with dwell time Dwell time, the period between when an attack occurs and when…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Event Stream Processing (ESP) has been a central component of CrowdStrike Falcon’s IOA approach since CrowdStrike's…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t For as long as we have had a cybersecurity industry, the market’s attention has been solidly…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     Users have long needed to access important resources such as virtual private networks (VPNs), web applications, and mail servers from anywhere in the world at any time. While the ability to access resources from anywhere is imperative for employees, threat actors often leverage stolen credentials to access systems and data. Due to large volumes of remote access connections, it can be difficult to distinguish between a legitimate and a malicious login. Today, we are releasing Once remote authentication activity is baselined across an environment, analysts can begin to identify authentication activity that deviates from business requirements and normalized patterns, such as: GeoLogonalyzer can help address these and similar situations by processing authentication logs containing timestamps, usernames, and source IP addresses. GeoLogonalyzer can be For a remote authentication log that records a source IP address, it is possible to estimate the location each logon originated from using data such as For example, if a user account, Meghan, logged on from New York City, New York on 2017-11-24 at 10:00:00 UTC and then logged on from Los Angeles, California 10 hours later on 2017-11-24 at 20:00:00 UTC, that is roughly a 2,450 mile change over 10 hours. Meghan’s logon source change can be normalized to 245 miles per hour which is reasonable through commercial airline travel. If a second user account, Harry, logged on from Dallas, Texas on 2017-11-25 at 17:00:00 UTC and then logged on from Sydney, Australia two hours later on 2017-11-25 at 19:00:00 UTC, that is roughly an 8,500 mile change over two hours. Harry’s logon source change can be normalized to 4,250 miles per hour, which is likely infeasible with modern travel technology. By focusing on the changes in logon sources, analysts do not have to manually review the many times that Harry might have logged in from Dallas before and after logging on from Sydney. Attackers understand that organizations may either be blocking or looking for connections from unexpected locations. One solution for attackers is to establish a proxy on either a compromised server in another country, or even through a rented server hosted in another country by companies such as AWS, DigitalOcean, or Choopa. Fortunately, Github user GeoLogonalyzer is designed to process remote access platform logs that include a timestamp, username, and source IP. Applicable log sources include, but are not limited to: GeoLogonalyzer’s built-in YYYY-MM-DD HH:MM:SS, username, source IP, optional source hostname, optional VPN client details GeoLogonalyzer’s code comments include instructions for adding customized log format support. Due to the various VPN log formats exported from VPN server manufacturers, version 1.0 of GeoLogonalyzer does not include support for raw VPN server logs. Figure 1 represents an example input GeoLogonalyzer.exe --csv VPNLogs.csv --output GeoLogonalyzedVPNLogs.csv python GeoLogonalyzer.py --csv VPNLogs.csv --output GeoLogonalyzedVPNLogs.csv Figure 2 represents the example output In the example output from Figure 2, GeoLogonalyzer helps identify the following anomalies in the Harry account’s logon patterns: Manual analysis of the data could also reveal anomalies such as: While it may be impossible to determine if a logon pattern is malicious based on this data alone, analysts can use GeoLogonalyzer to flag and investigate potentially suspicious logon activity through other investigative methods. Any RFC1918 source IP addresses, such as 192.168.X.X and 10.X.X.X, will not have a physical location registered in the MaxMind database. By default, GeoLogonalyzer will use the coordinates (0, 0) for any reserved IP address, which may alter results. Analysts can manually edit these coordinates, if desired, by modifying the RESERVED_IP_COORDINATES constant in the Python script. Setting this constant to the coordinates of your office location may provide the most accurate results, although may not be feasible if your organization has multiple locations or other point-to-point connections. GeoLogonalyzer also accepts the parameter –skip_rfc1918, which will completely ignore any RFC1918 source IP addresses and could result in missed activity. It may also be useful to include failed logon attempts and logoff records with the log source data to see anomalies related to source information of all VPN activity. At this time, GeoLogonalyzer does not distinguish between successful logons, failed logon attempts, and logoff events. GeoLogonalyzer also does not detect overlapping logon sessions from multiple source IP addresses. Note that the use of VPN or other tunneling services may create false positives. For example, a user may access an application from their home office in Wyoming at 08:00 UTC, connect to a VPN service hosted in Georgia at 08:30 UTC, and access the application again through the VPN service at 09:00 UTC. GeoLogonalyzer would process this application access log and detect that the user account required a FAST travel rate of roughly 1,250 miles per hour which may appear malicious. Establishing a baseline of legitimate authentication patterns is recommended to understand false positives. GeoLogonalyzer relies on open source data to make cloud hosting provider determinations. These lookups are only as accurate as the available open source data. Understanding that no single analysis method is perfect, the following recommendations can help security teams prevent the abuse of remote access platforms and investigate suspected compromise. Download Christopher Schmitt, Seth Summersett, Jeff Johns, and Alexander Mulfinger.  I wanted to create a how-to blog post about creating gephi visualizations, but I realized it’d probably need to include, like, a thousand embedded screenshots. So I made a video instead.For those not keeping track, the NIST Cybersecurity Framework received its first update on April 16, 2018. If you’re already familiar with the original 2014 version, fear not. Everything you know and love about version 1.0 remains in 1.1, along with a few helpful additions and clarifications. Among the most important clarifications, one in particular jumps out: If your company thought it complied with the old Framework and intends to comply with the new one, think again. Your company hasn’t been in compliance with the Framework, and it never will be. Why?  Because NIST says so. According to NIST, although companies can comply with their own cybersecurity requirements, and they can use the Framework to determine and express those requirements, there is no such thing as complying with the Framework itself. In the words of NIST, saying otherwise is confusing. The Framework should instead be “used” and “leveraged.” Which leads us to a second important clarification, this time concerning the Framework Core.  Perhaps you know the Core by its less illustrious name: “Appendix A.” Regardless, the Core is a 20-page spreadsheet that lists five Functions (Identify, Protect, Detect, Respond, and Recover); dozens of cybersecurity categories and subcategories, including such classics as “anomalous activity is detected;” and, provides Informative References of common standards, guidelines, and practices. Practitioners tend to agree that the Core is an invaluable resource when used correctly. For NIST, proper use requires that companies view the Core as a collection of potential “outcomes” to achieve rather than a checklist of “actions” to perform. Expressed differently, the Core outlines the objectives a company may wish to pursue, while providing flexibility in terms of how, and even whether, to accomplish them. So, why are these particular clarifications worthy of mention? Simply put, because they demonstrate that NIST continues to hold firm to risk-based management principles. A company cannot merely hand the NIST Framework over to its security team and tell it to check the boxes and issue a certificate of compliance. Instead, to use NIST’s words: “The Framework focuses on using business drivers to guide cybersecurity activities and considering cybersecurity risks as part of the organization’s risk management processes.” Wait, what?  That sentence is worth a second read. Its importance lies in the fact that NIST is not encouraging companies to achieve every Core outcome. Instead, organizations are expected to consider their business requirements and material risks, and then make reasonable and informed cybersecurity decisions using the Framework to help them identify and prioritize feasible and cost-effective improvements. Which leads us to discuss a particularly important addition to version 1.1. The new Framework now includes a section titled “Self-Assessing Cybersecurity Risk with the Framework.” In fact, that’s the only entirely new section of the document. Companies are encouraged to perform internal or third-party assessments using the Framework. Leading this effort requires sufficient expertise in order to accurately inform an organization of its current cybersecurity risk profile, foster discussions that lead to an agreement on the desired or “target” profile, and drive the organization’s adoption and execution of a remediation plan to address material gaps between what the company has in place and what it needs. Of course, there are many other additions to the Framework (most prominently, a stronger focus on Yes, you read that last part right, “evolution activities.” To avoid corporate extinction in today’s data- and technology-driven landscape, a famous Jack Welch quote comes to mind: “Change before you have to.” Considering its resounding adoption not only within the United States, but in other parts of the world, as well, the best time to incorporate the Framework and its revisions into your enterprise risk management program is now. Finally, if you need help assessing your cybersecurity posture and leveraging the Framework, reach out. As the old adage goes, you don’t need to know everything. You just need to know where to find what you need when you need it.       Steven Chabinsky is global chair of the Data, Privacy, and Cybersecurity practice at White  Case LLP, an international law firm, and the cyber tactics columnist for Security magazine. He previously served as a member of the President’s Commission on Enhancing National Cybersecurity, the General Counsel and Chief Risk Officer of CrowdStrike, and Deputy Assistant Director of the FBI Cyber Division. Learn more: https://www.whitecase.com/people/steven-r-chabinsky \t\t\t   \t\t \t\t \t    \tAt the Gartner Security and Risk Management Summit 2018 in June, CrowdStrike CTO and Co-Founder Dmitri…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t In an age where information is the ultimate currency, traditional strategies focused on malware, perimeter defense,…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t A comprehensive Next-Generation Endpoint Protection strategy shouldn’t just be about reacting and responding to threats, but…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     Gartner just released its inaugural The chart below shows vendor product scores for use case Type A or “forward leaning”  organizations: The Critical Capabilities research is a continuation of the analysis conducted for the CrowdStrike believes the Critical Capabilities report is an important complement to the Gartner MQ for EPP because the former focuses solely on product capabilities, rather than on factors such as vendor market share. More specifically, the Critical Capabilities report helps organizations understand which vendors offer the solutions that will best fit their needs by segmenting the assessments into different use cases. We firmly believe the reason CrowdStrike scored so high in all use cases outlined by Gartner is because of our modular architecture, which offers customers the flexibility to choose the best solution to fit their needs. The CrowdStrike Falcon® platform unifies and delivers IT Hygiene, next-generation antivirus (NGAV), endpoint detection and response (EDR), managed threat hunting, and threat intelligence — all via a single lightweight agent. We urge organizations to review both reports —  the For a free trial of CrowdStrike Falcon Prevent™ next-gen AV, clickThe Health Insurance Portability and Accountability Act (HIPAA) has big consequences for organizations of all sizes.…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Global statistics in the most recent Ponemon report on the cost of a data breach show…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t A recent interview with CrowdStrike VP of Product Marketing Dan Larson, for the CyberWire Daily Podcast,…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     At Zebra Technologies, our global information security strategy is to provide “security visibility that’s visionary.” We empower employees to do what they need to do with the freedoms enabled by mobile and cloud computing, while maintaining visibility into their computing assets so we can keep them and our business interests secure. Our cybersecurity tools provide the visibility. We place agents and monitoring software onto employees’ computers so they’re free to go wherever they need to go. No matter where they are, no matter what they do, we can protect them. To make this approach work, our security staff must understand our organization, its business and its goals, and to make sure security is a part of the puzzle, not an outlier. And we also work closely with our IT team to make sure that programs are delivered and operating in a secure manner. In doing so, our security team has become a “department of yes” that helps our business achieve both its goals and a reasonable level of security. Essentially, our security and IT departments have become business partners, and having security tools that can do “more than just security” is an important part of that relationship. The CrowdStrike® Falcon® platform is a great example of this concept, because it’s more than just a security tool that safeguards our systems and staff. It’s also a vital operational tool for our IT team. For example, the Falcon platform provides visibility into our assets — what operating systems are being used, what software versions are running, things like that. It helps us from an asset inventory and management perspective. Of the numerous ways the Falcon platform benefits Zebra Technologies, the following are key: To integrate cybersecurity into the broader IT department with the aim of enabling business, might I suggest the following: The benefits of this “visionary visibility” security approach extend to customer relationships and add market value to the company, as well, because it can enable a business to expand and try new technologies. At the end of the day, having a security platform that can do “more than just security” is a benefit to the entire organization. We’re all here to make sure that Zebra Technologies succeeds, and that we have the right tools in place to do so. Prior to co-founding CrowdStrike®, I was probably best known in the business world as one of…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t We started CrowdStrike five years ago with the mission to revolutionize endpoint security. At that time,…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Increasing use of nation-state tactics and tools poses a growing threat for companies that must be…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     Although many industries have long offered product warranties to assure customers the products they purchase will function as advertised, this has not been true for cybersecurity. When a security product fails, customers have had little recourse — until now. CrowdStrike® is thrilled to be changing the game once again by offering our customers a $1 million dollar warranty on our most comprehensive solution, CrowdStrike is so confident in Falcon EPP Complete’s breach protection capabilities that we have established a breach warranty of up to $1 million in the event that a customer using EPP Complete experiences a breach within their protected environment that EPP Complete should have prevented. If a legitimate breach occurs, we’ve made the warranty easy to implement, without unachievable requirements or hidden caveats. And the beauty is that the warranty is included in the purchase price of the product. All new EPP Complete clients are eligible for this warranty. If the warranty is triggered, it provides a broad spectrum of benefits that cover the following breach response expenses:  incident response, legal fees, notification, credit monitoring, forensic investigation and public communications expenses. Also, for customers who are developing an overall cyber risk management program that includes a balance between cyber risk mitigation and cyber risk transfer, Falcon EPP Complete is the ideal solution. If a Falcon EPP Complete customer experiences a breach, the breach prevention warranty transfers risk from the customer to CrowdStrike. The benefits for CrowdStrike customers are self-evident: The warranty provides an extra layer of protection at no additional cost. The combination of the efficacy and simplicity of EPP Complete with the CrowdStrike warranty gives our customers ultimate peace-of-mind, relieving anxiety and financial loss if an unexpected breach occurs, and making the breach response process more convenient, efficient, and stress-free. With this warranty for CrowdStrike Falcon EPP Complete, we are demonstrating our confidence in the most tangible way possible: by giving customers the peace-of-mind and financial assurance they deserve. Technology expert and author Dan Woods has published a series of articles in Forbes magazine that…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t In this blog post, we’ll take a look at an example of a typical targeted attack…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t CrowdStrike® is proud to announce that it is officially “In Process” for the Federal Risk and…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     Many cyber incidents can be traced back to an original alert that was either missed or ignored by the Security Operations Center (SOC) or Incident Response (IR) team. While most analysts and SOCs are vigilant and responsive, the fact is they are often overwhelmed with alerts. If a SOC is unable to review all the alerts it generates, then sooner or later, something important will slip through the cracks. The core issue here is scalability. It is far easier to create more alerts than to create more analysts, and the cyber security industry is far better at alert generation than resolution. More intel feeds, more tools, and more visibility all add to the flood of alerts. There are things that SOCs can and should do to manage this flood, such as increasing automation of forensic tasks (pulling PCAP and acquiring files, for example) and using aggregation filters to group alerts into similar batches. These are effective strategies and will help reduce the number of required actions a SOC analyst must take. However, the In this blog post, we propose machine learning based strategies to help mitigate this bottleneck and take back control of the SOC. We have implemented these strategies in our FireEye Managed Defense SOC, and our analysts are taking advantage of this approach within their alert triaging workflow. In the following sections, we will describe our process to collect data, capture alert analysis, create a model, and build an efficacy workflow – all with the ultimate goal of automating alert triage and freeing up analyst time. Every alert that comes into a SOC environment contains certain bits of information that an analyst uses to determine if the alert represents malicious activity. Often, there are well-paved analytical processes and pathways used when evaluating these forensic artifacts over time. We wanted to explore if, in an effort to truly scale our SOC operations, we could extract these analytical pathways, train a machine to traverse them, and potentially discover new ones. Think of a SOC as a self-contained machine that inputs unlabeled alerts and outputs the alerts labeled as “malicious” or “benign”. How can we capture the analysis and determine that something is indeed malicious, and then recreate that analysis at scale? In other words, what if we could train a machine to make the same analytical decisions as an analyst, within an acceptable level of confidence? The data science term for this is a “Supervised Classification Model”. It is “supervised” in the sense that it learns by being shown data already labeled as benign or malicious, and it is a “classification model” in the sense that once it has been trained, we want it to look at a new piece of data and make a decision between one of several discrete outcomes. In our case, we only want it to decide between two “classes” of alerts: malicious and benign. In order to begin creating such a model, a dataset must be collected. This dataset forms the “experience” of the model, and is the information we will use to “train” the model to make decisions. In order to supervise the model, each unit of data must be labeled as either malicious or benign, so that the model can evaluate each observation and begin to figure out what makes something malicious versus what makes it benign. Typically, collecting a clean, labeled dataset is one of the hardest parts of the supervised model pipeline; however, in the case of our SOC, our analysts are constantly triaging (or “labeling”) thousands of alerts every week, and so we were lucky to have an abundance of clean, standardized, labeled alerts. Once a labeled dataset has been defined, the next step is to define “features” that can be used to portray the information resident in each alert. A “feature” can be thought of as an aspect of a bit of information. For example, if the information is represented as a string, a natural “feature” could be the length of the string. The central goal idea behind building features for our alert classification model was to find a way to represent and record all the aspects that an analyst might consider when making a decision. Building the model then requires choosing a model structure to use, and training the model on a subset of the total data available. The larger and more diverse the training data set, generally the better the model will perform. The remaining data is used as a “test set” to see if the trained model is indeed effective. Holding out this test set ensures the model is evaluated on samples it has never seen before, but for which the true labels are known. Finally, it is critical to ensure there is a way to evaluate the efficacy of the model over time, as well as to investigate mistakes so that appropriate adjustments can be made. Without a plan and a pipeline to evaluate and retrain, the model will almost certainly decay in performance. Before creating any of our own models, we interviewed experienced analysts and documented the information they typically evaluate before making a decision on an alert. Those interviews formed the basis of our feature extraction. For example, when an analyst says that reviewing an alert is “easy”, we ask: “Why? And what helps you make that decision?” It is this reverse engineering of sorts that gives insight into features and models we can use to capture analysis. For example, consider a process execution event. An alert on a potentially malicious process execution may contain the following fields: While this may initially seem like a limited feature space, there is a lot of useful information that one can extract from these fields. Beginning with the process path of, say, “C:\\windows\\temp\\m.exe”, an analyst can immediately see some features: While these may seem simple, over a vast amount of data and examples, extracting these bits of information will help the model to differentiate between events. Even the most basic aspects of an artifact must be captured in order to “teach” the model to view processes the way an analyst does. The features are then encoded into a more discrete representation, similar to this: Temp_folder  Depth  Name_Length  Extension  common_process_name   TRUE  2  1  exe  FALSE     Another important feature to consider about a process execution event is the combination of parent process and child process. Deviation from expected “lineage” can be a strong indicator of malicious activity. Say the parent process of the aforementioned example was ‘powershell.exe’. Potential new features could then be derived from the concatenation of the parent process and the process itself: ‘powershell.exe_m.exe’. This functionally serves as an identity for the parent-child relation and captures another key analysis artifact. The richest field, however, is probably the process arguments. Process arguments are their own sort of language, and language analysis is a well-tread space in predictive analytics. We can look for things including, but not limited to: The way these features and their values appear in a training dataset will define the way the model learns. Based on the distribution of features across thousands of alerts, relationships will start to emerge between features and labels. These relationships will then be recorded in our model, and ultimately used to influence the predictions for new alerts. Looking at distributions of features in the training set can give insight into some of these potential relationships. For example, Figure 2 shows how the distribution of Process Command Length may appear when grouping by malicious (red) and benign (blue). This graph shows that over a subset of samples, the longer the command length, the more likely it is to be malicious. This manifests as red on the right and blue on the left. However, process length is not the only factor. As part of our feature set, we also thought it would be useful to approximate the “complexity” of each command. For this, we used “Figure 3 shows a distribution of command entropy, broken out into malicious and benign. While the classes do not separate entirely, we can see that for this sample of data, samples with higher entropy generally have a higher chance of being malicious. Once features have been generated for the whole dataset, it is time to use them to train a model. There is no perfect procedure for picking the best model, but looking at the type of features in our data can help narrow it down. In the case of a process event, we have a combination of features represented as strings and numbers. When an analyst evaluates each artifact, they ask questions about each of these features, and combine the answers to estimate the probability that the process is malicious. For our use case, it also made sense to prioritize an ‘interpretable’ model – that is, one that can more easily expose why it made a certain decision about an artifact. This way analysts can build confidence in the model, as well as detect and fix analytical mistakes that the model is making. Given the nature of the data, the decisions analysts make, and the desire for interpretability, we felt that a decision tree-based model would be well-suited for alert classification. There are many publicly available resources to learn about decision trees, but the basic intuition behind a decision tree is that it is an iterative process, asking a series of questions to try to arrive at a highly confident answer. Anyone who has played the game “Twenty Questions” is familiar with this concept. Initially, general questions are asked to help eliminate possibilities, and then more specific questions are asked to narrow down the possibilities. After enough questions are asked and answered, the ‘questioner’ feels they have a high probability of guessing the right answer. Figure 4 shows an example of a decision tree that one might use to evaluate process executions. For the example alert in the diagram, the “decision path” is marked in red. This is how this decision tree model makes a prediction. It first asks: “Is the length greater than 100 characters?” If so, it moves to the next question “Does it contain the string ‘http’?” and so on until it feels confident in making an educated guess. In the example in Figure 4, given that 95 percent of all the training alerts traveling this decision path were malicious, the model predicts a 95 percent chance that this alert will also be malicious. Because they can ask such detailed combinations of questions, it is possible that decision trees can “overfit”, or learn rules that are too closely tied to the training set. This reduces the model’s ability to “generalize” to new data. One way to mitigate this effect is to use many slightly different decision trees and have them each “vote” on the outcome. This “ensemble” of decision trees is called a Random Forest, and it can improve performance for the model when deployed in the wild. This is the algorithm we ultimately chose for our model. When a new alert appears, the data in the artifact is transformed into a vector of the encoded features, with the same structure as the feature representations used to train the model. The model then evaluates this “feature vector” and applies a confidence level for the predicted label. Based on thresholds we set, we can then classify the alert as malicious or benign. As an example, the event shown in Figure 5 might create the following feature values: Based on how they were trained, the trees in the model each ask a series of questions of the new feature vector. As the feature vector traverses each tree, it eventually converges on a terminal “leaf” classifying it as either benign or malicious. We can then evaluate the aggregated decisions made by each tree to estimate which features in the vector played the largest role in the ultimate classification. For the analysts in the SOC, we then present the features extracted from the model, showing the distribution of those features over the entire dataset. This gives the analysts insight into “why” the model thought what it thought, and how those features are represented across all alerts we have seen. For example, the “explanation” for this alert might look like: Thus, at the time of analysis, the analysts can see the raw data of the event, the prediction from the model, an approximation of the decision path, and a simplified, interpretable view of the overall feature importance. Showing the features the model used to reach the conclusion allows experienced analysts to compare their approach with the model, and give feedback if the model is doing something wrong. Conversely, a new analyst may learn to look at features they may have otherwise missed: the parent-child relationship, signs of obfuscation, or network connection strings in the arguments. After all, the model has learned on the collective experience of every analyst over thousands of alerts. Therefore, the model provides an actionable reflection of the aggregate analyst experience Additionally, it is possible to write rules using the output of the model as a parameter. If the model is particularly confident on a subset of alerts, and the SOC feels comfortable automatically classifying that family of threats, it is possible to simply write a rule to say: “If the alert is of this type, AND for this malware family, AND the model confidence is above 99, automatically call this alert bad and generate a report.” Or, if there is a storm of probable false positives, one could write a rule to cull the herd of false positives using a model score below 10. The day the model is trained, it stops learning. However, threats – and therefore alerts – are constantly evolving. Thus, it is imperative to continually retrain the model with new alert data to ensure it continues to learn from changes in the environment. Additionally, it is critical to monitor the overall efficacy of the model over time. Building an efficacy analysis pipeline to compare model results against analyst feedback will help identify if the model is beginning to drift or develop structural biases. Evaluating and incorporating analyst feedback is also critical to identify and address specific misclassifications, and discover potential new features that may be necessary. To accomplish these goals, we run a background job that updates our training database with newly labeled events. As we get more and more alerts, we periodically retrain our model with the new observations. If we encounter issues with accuracy, we diagnose and work to address them. Once we are satisfied with the overall accuracy score of our retrained model, we store the model object and begin using that model version. We also provide a feedback mechanism for analysts to record when the model is wrong. An analyst can look at the label provided by the model and the explanation, but can also make their own decision. Whether they agree with the model or not, they can input their own label through the interface. We store this label provided by the analyst along with any optional explanation given by them regarding the explanation. Finally, it should be noted that these manual labels may require further evaluation. As an example, consider a commodity malware alert, in which network command and control communications were sinkholed. An analyst may evaluate the alert, pull back triage details, including PCAP samples, and see that while the malware executed, the true Monitoring efficacy over time, updating and retraining the model with new alerts, and evaluating manual analyst feedback gives us visibility into how the model is performing and learning over time. Ultimately this helps to build confidence in the model, so we can automate more tasks and free up analyst time to perform tasks such as hunting and investigation. A supervised learning model is not a replacement for an experienced analyst. However, incorporating predictive analytics and machine learning into the SOC workflow can help augment the productivity of analysts, free up time, and ensure they utilize investigative skills and creativity on the threats that truly require expertise. This blog post outlines the major components and considerations of building an alert classification model for the SOC. Data collection, labeling, feature generation, model training, and efficacy analysis must all be carefully considered when building such a model. FireEye continues to iterate on this research to improve our detection and response capabilities, continually improve the detection efficacy of our products, and ultimately protect our clients. The process and examples shown discussed in this post are not mere research. Within our A big thank you to Seth Summersett and Clara Brooks. *** The FireEye ICE Data Science Team is a small, highly trained team of data scientists and engineers, focused on delivering impactful capabilities to our analysts, products, and customers. ICE-DS is always looking for exceptional candidates interested in researching and solving difficult problems in cybersecurity. If you’re interested, check out Many cyber incidents can be traced back to an original alert that was either missed or ignored by the Security Operations Center (SOC) or Incident Response (IR) team. While most analysts and SOCs are vigilant and responsive, the fact is they are often overwhelmed with alerts. If a SOC is unable to review all the alerts it generates, then sooner or later, something important will slip through the cracks. The core issue here is scalability. It is far easier to create more alerts than to create more analysts, and the cyber security industry is far better at alert generation than resolution. More intel feeds, more tools, and more visibility all add to the flood of alerts. There are things that SOCs can and should do to manage this flood, such as increasing automation of forensic tasks (pulling PCAP and acquiring files, for example) and using aggregation filters to group alerts into similar batches. These are effective strategies and will help reduce the number of required actions a SOC analyst must take. However, the In this blog post, we propose machine learning based strategies to help mitigate this bottleneck and take back control of the SOC. We have implemented these strategies in our FireEye Managed Defense SOC, and our analysts are taking advantage of this approach within their alert triaging workflow. In the following sections, we will describe our process to collect data, capture alert analysis, create a model, and build an efficacy workflow – all with the ultimate goal of automating alert triage and freeing up analyst time. Every alert that comes into a SOC environment contains certain bits of information that an analyst uses to determine if the alert represents malicious activity. Often, there are well-paved analytical processes and pathways used when evaluating these forensic artifacts over time. We wanted to explore if, in an effort to truly scale our SOC operations, we could extract these analytical pathways, train a machine to traverse them, and potentially discover new ones. Think of a SOC as a self-contained machine that inputs unlabeled alerts and outputs the alerts labeled as “malicious” or “benign”. How can we capture the analysis and determine that something is indeed malicious, and then recreate that analysis at scale? In other words, what if we could train a machine to make the same analytical decisions as an analyst, within an acceptable level of confidence? The data science term for this is a “Supervised Classification Model”. It is “supervised” in the sense that it learns by being shown data already labeled as benign or malicious, and it is a “classification model” in the sense that once it has been trained, we want it to look at a new piece of data and make a decision between one of several discrete outcomes. In our case, we only want it to decide between two “classes” of alerts: malicious and benign. In order to begin creating such a model, a dataset must be collected. This dataset forms the “experience” of the model, and is the information we will use to “train” the model to make decisions. In order to supervise the model, each unit of data must be labeled as either malicious or benign, so that the model can evaluate each observation and begin to figure out what makes something malicious versus what makes it benign. Typically, collecting a clean, labeled dataset is one of the hardest parts of the supervised model pipeline; however, in the case of our SOC, our analysts are constantly triaging (or “labeling”) thousands of alerts every week, and so we were lucky to have an abundance of clean, standardized, labeled alerts. Once a labeled dataset has been defined, the next step is to define “features” that can be used to portray the information resident in each alert. A “feature” can be thought of as an aspect of a bit of information. For example, if the information is represented as a string, a natural “feature” could be the length of the string. The central idea behind building features for our alert classification model was to find a way to represent and record all the aspects that an analyst might consider when making a decision. Building the model then requires choosing a model structure to use, and training the model on a subset of the total data available. The larger and more diverse the training data set, generally the better the model will perform. The remaining data is used as a “test set” to see if the trained model is indeed effective. Holding out this test set ensures the model is evaluated on samples it has never seen before, but for which the true labels are known. Finally, it is critical to ensure there is a way to evaluate the efficacy of the model over time, as well as to investigate mistakes so that appropriate adjustments can be made. Without a plan and a pipeline to evaluate and retrain, the model will almost certainly decay in performance. Before creating any of our own models, we interviewed experienced analysts and documented the information they typically evaluate before making a decision on an alert. Those interviews formed the basis of our feature extraction. For example, when an analyst says that reviewing an alert is “easy”, we ask: “Why? And what helps you make that decision?” It is this reverse engineering of sorts that gives insight into features and models we can use to capture analysis. For example, consider a process execution event. An alert on a potentially malicious process execution may contain the following fields: While this may initially seem like a limited feature space, there is a lot of useful information that one can extract from these fields. Beginning with the process path of, say, “C:\\windows\\temp\\m.exe”, an analyst can immediately see some features: While these may seem simple, over a vast amount of data and examples, extracting these bits of information will help the model to differentiate between events. Even the most basic aspects of an artifact must be captured in order to “teach” the model to view processes the way an analyst does. The features are then encoded into a more discrete representation, similar to this: Temp_folder  Depth  Name_Length  Extension  common_process_name   TRUE  2  1  exe  FALSE     Another important feature to consider about a process execution event is the combination of parent process and child process. Deviation from expected “lineage” can be a strong indicator of malicious activity. Say the parent process of the aforementioned example was ‘powershell.exe’. Potential new features could then be derived from the concatenation of the parent process and the process itself: ‘powershell.exe_m.exe’. This functionally serves as an identity for the parent-child relation and captures another key analysis artifact. The richest field, however, is probably the process arguments. Process arguments are their own sort of language, and language analysis is a well-tread space in predictive analytics. We can look for things including, but not limited to: The way these features and their values appear in a training dataset will define the way the model learns. Based on the distribution of features across thousands of alerts, relationships will start to emerge between features and labels. These relationships will then be recorded in our model, and ultimately used to influence the predictions for new alerts. Looking at distributions of features in the training set can give insight into some of these potential relationships. For example, Figure 2 shows how the distribution of Process Command Length may appear when grouping by malicious (red) and benign (blue). This graph shows that over a subset of samples, the longer the command length, the more likely it is to be malicious. This manifests as red on the right and blue on the left. However, process length is not the only factor. As part of our feature set, we also thought it would be useful to approximate the “complexity” of each command. For this, we used “Figure 3 shows a distribution of command entropy, broken out into malicious and benign. While the classes do not separate entirely, we can see that for this sample of data, samples with higher entropy generally have a higher chance of being malicious. Once features have been generated for the whole dataset, it is time to use them to train a model. There is no perfect procedure for picking the best model, but looking at the type of features in our data can help narrow it down. In the case of a process event, we have a combination of features represented as strings and numbers. When an analyst evaluates each artifact, they ask questions about each of these features, and combine the answers to estimate the probability that the process is malicious. For our use case, it also made sense to prioritize an ‘interpretable’ model – that is, one that can more easily expose why it made a certain decision about an artifact. This way analysts can build confidence in the model, as well as detect and fix analytical mistakes that the model is making. Given the nature of the data, the decisions analysts make, and the desire for interpretability, we felt that a decision tree-based model would be well-suited for alert classification. There are many publicly available resources to learn about decision trees, but the basic intuition behind a decision tree is that it is an iterative process, asking a series of questions to try to arrive at a highly confident answer. Anyone who has played the game “Twenty Questions” is familiar with this concept. Initially, general questions are asked to help eliminate possibilities, and then more specific questions are asked to narrow down the possibilities. After enough questions are asked and answered, the ‘questioner’ feels they have a high probability of guessing the right answer. Figure 4 shows an example of a decision tree that one might use to evaluate process executions. For the example alert in the diagram, the “decision path” is marked in red. This is how this decision tree model makes a prediction. It first asks: “Is the length greater than 100 characters?” If so, it moves to the next question “Does it contain the string ‘http’?” and so on until it feels confident in making an educated guess. In the example in Figure 4, given that 95 percent of all the training alerts traveling this decision path were malicious, the model predicts a 95 percent chance that this alert will also be malicious. Because they can ask such detailed combinations of questions, it is possible that decision trees can “overfit”, or learn rules that are too closely tied to the training set. This reduces the model’s ability to “generalize” to new data. One way to mitigate this effect is to use many slightly different decision trees and have them each “vote” on the outcome. This “ensemble” of decision trees is called a Random Forest, and it can improve performance for the model when deployed in the wild. This is the algorithm we ultimately chose for our model. When a new alert appears, the data in the artifact is transformed into a vector of the encoded features, with the same structure as the feature representations used to train the model. The model then evaluates this “feature vector” and applies a confidence level for the predicted label. Based on thresholds we set, we can then classify the alert as malicious or benign. As an example, the event shown in Figure 5 might create the following feature values: Based on how they were trained, the trees in the model each ask a series of questions of the new feature vector. As the feature vector traverses each tree, it eventually converges on a terminal “leaf” classifying it as either benign or malicious. We can then evaluate the aggregated decisions made by each tree to estimate which features in the vector played the largest role in the ultimate classification. For the analysts in the SOC, we then present the features extracted from the model, showing the distribution of those features over the entire dataset. This gives the analysts insight into “why” the model thought what it thought, and how those features are represented across all alerts we have seen. For example, the “explanation” for this alert might look like: Thus, at the time of analysis, the analysts can see the raw data of the event, the prediction from the model, an approximation of the decision path, and a simplified, interpretable view of the overall feature importance. Showing the features the model used to reach the conclusion allows experienced analysts to compare their approach with the model, and give feedback if the model is doing something wrong. Conversely, a new analyst may learn to look at features they may have otherwise missed: the parent-child relationship, signs of obfuscation, or network connection strings in the arguments. After all, the model has learned on the collective experience of every analyst over thousands of alerts. Therefore, the model provides an actionable reflection of the aggregate analyst experience Additionally, it is possible to write rules using the output of the model as a parameter. If the model is particularly confident on a subset of alerts, and the SOC feels comfortable automatically classifying that family of threats, it is possible to simply write a rule to say: “If the alert is of this type, AND for this malware family, AND the model confidence is above 99, automatically call this alert bad and generate a report.” Or, if there is a storm of probable false positives, one could write a rule to cull the herd of false positives using a model score below 10. The day the model is trained, it stops learning. However, threats – and therefore alerts – are constantly evolving. Thus, it is imperative to continually retrain the model with new alert data to ensure it continues to learn from changes in the environment. Additionally, it is critical to monitor the overall efficacy of the model over time. Building an efficacy analysis pipeline to compare model results against analyst feedback will help identify if the model is beginning to drift or develop structural biases. Evaluating and incorporating analyst feedback is also critical to identify and address specific misclassifications, and discover potential new features that may be necessary. To accomplish these goals, we run a background job that updates our training database with newly labeled events. As we get more and more alerts, we periodically retrain our model with the new observations. If we encounter issues with accuracy, we diagnose and work to address them. Once we are satisfied with the overall accuracy score of our retrained model, we store the model object and begin using that model version. We also provide a feedback mechanism for analysts to record when the model is wrong. An analyst can look at the label provided by the model and the explanation, but can also make their own decision. Whether they agree with the model or not, they can input their own label through the interface. We store this label provided by the analyst along with any optional explanation given by them regarding the explanation. Finally, it should be noted that these manual labels may require further evaluation. As an example, consider a commodity malware alert, in which network command and control communications were sinkholed. An analyst may evaluate the alert, pull back triage details, including PCAP samples, and see that while the malware executed, the true Monitoring efficacy over time, updating and retraining the model with new alerts, and evaluating manual analyst feedback gives us visibility into how the model is performing and learning over time. Ultimately this helps to build confidence in the model, so we can automate more tasks and free up analyst time to perform tasks such as hunting and investigation. A supervised learning model is not a replacement for an experienced analyst. However, incorporating predictive analytics and machine learning into the SOC workflow can help augment the productivity of analysts, free up time, and ensure they utilize investigative skills and creativity on the threats that truly require expertise. This blog post outlines the major components and considerations of building an alert classification model for the SOC. Data collection, labeling, feature generation, model training, and efficacy analysis must all be carefully considered when building such a model. FireEye continues to iterate on this research to improve our detection and response capabilities, continually improve the detection efficacy of our products, and ultimately protect our clients. The process and examples shown discussed in this post are not mere research. Within our A big thank you to Seth Summersett and Clara Brooks. *** The FireEye ICE Data Science Team is a small, highly trained team of data scientists and engineers, focused on delivering impactful capabilities to our analysts, products, and customers. ICE-DS is always looking for exceptional candidates interested in researching and solving difficult problems in cybersecurity. If you’re interested, check out        In February 2014 at the RSA Conference, my colleague George Kurtz and I presented a session…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"Visionaries not only believe that the impossible can be done, but that it must be done.\"…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t It has been nearly three weeks since the announcement on September 25th of the landmark Cyber…\t\t\t  \t\t   \t \t\t\t\t\t\t\t\t\t   \t     In December 2017, FireEye's Despite the routine techniques employed to gain access to an OT environment, the threat actors behind the TRITON malware framework invested significant time learning about the Triconex Safety Instrumented System (SIS) controllers and TriStation, a proprietary network communications protocol. The investment and purpose of the Triconex SIS controllers leads Mandiant to assess the attacker's objective was likely to build the capability to cause physical consequences. TriStation remains closed source and there is no official public information detailing the structure of the protocol, raising several questions about how the TRITON framework was developed. Did the actor have access to a Triconex controller and TriStation 1131 software suite? When did development first start? How did the threat actor reverse engineer the protocol, and to what extent? What is the protocol structure? FireEye’s Advanced Practices Team was born to investigate adversary methodologies, and to answer these types of questions, so we started with a deeper look at the TRITON’s own Python scripts. Glossary: TriStation is a proprietary network protocol and there is no public documentation detailing its structure or how to create software applications that use TriStation. The current TriStation UDP/IP protocol is little understood, but natively implemented through the TriStation 1131 software suite. TriStation operates by UDP over port 1502 and allows for communications between designated masters (PCs with the software that are “engineering workstations”) and slaves (Triconex controllers with special communications modules) over a network. To us, the Triconex systems, software and associated terminology sound foreign and complicated, and the TriStation protocol is no different. Attempting to understand the protocol from ground zero would take a considerable amount of time and reverse engineering effort – so why not learn from TRITON itself? With the TRITON framework containing TriStation communication functionality, we pursued studying the framework to better understand this mysterious protocol. Work smarter, not harder, amirite? The TsLow.pyc (Figure 1) contains several pieces of code for error handling, but these also present some cues to the protocol structure. In the TsLow.pyc’s function for print_last_error we see error handling for “TCM Error”. This compares the TriStation packet value at offset 0 with a value in a corresponding array from TS_cnames.pyc (Figure 2), which is largely used as a “dictionary” for the protocol. From this we can infer that offset 0 of the TriStation protocol contains message types. This is supported by an additional function, tcm_result, which declares type, size = struct.unpack('HH', data_received[0:4]), stating that the first two bytes should be handled as integer type and the second two bytes are integer size of the TriStation message. This is our first glimpse into what the threat actor(s) understood about the TriStation protocol. Since there are only 11 defined message types, it really doesn't matter much if the type is one byte or two because the second byte will always be 0x00. We also have indications that message type 5 is for all Execution Command Requests and Responses, so it is curious to observe that the TRITON developers called this “Command Reply.” (We won’t understand this naming convention until later.) Next we examine TsLow.pyc’s print_last_error function (Figure 3) to look at “TS Error” and “TS_names.” We begin by looking at the ts_err variable and see that it references ts_result. We follow that thread to ts_result, which defines a few variables in the next 10 bytes (Figure 4): dir, cid, cmd, cnt, unk, cks, siz = struct.unpack(', ts_packet[0:10]). Now things are heating up. What fun. There’s a lot to unpack here, but the most interesting thing is how this piece script breaks down 10 bytes from ts_packet into different variables. Referencing tcm_result (Figure 5) we see that it defines type and size as the first four bytes (offset 0 – 3) and tcm_result returns the packet bytes 4:-2 (offset 4 to the end minus 2, because the last two bytes are the CRC-16 checksum). Now that we know where tcm_result leaves off, we know that the ts_reply “cmd” is a single byte at offset 6, and corresponds to the values in the TS_cnames.pyc array and TS_names (Figure 6). The TRITON script also tells us that any integer value over 100 is a likely “command reply.” Sweet. When looking back at the ts_result packet header definitions, we begin to see some gaps in the TRITON developer's knowledge: dir, cid, cmd, cnt, unk, cks, siz = struct.unpack(', ts_packet[0:10]). We're clearly speculating based on naming conventions, but we get an impression that offsets 4, 5 and 6 could be \"direction\", \"controller ID\" and \"command\", respectively. Values such as \"unk\" show that the developer either did not know or did not care to identify this value. We suspect it is a constant, but this value is still unknown to us. The TRITON threat actor’s knowledge and reverse engineering effort provides us a better understanding of the protocol. From here we can start to form a more complete picture and document the basic functionality of TriStation. We are primarily interested in message type 5, Execution Command, which best illustrates the overall structure of the protocol. Other, smaller message types will have varying structure. Minute discrepancies aside, the TriStation structure detailed in Figure 7 is supported by other public analyses. Foremost, researchers from the Coordinated Science Laboratory (CSL) at University of Illinois at Urbana-Champaign published a 2017 paper titled \"There are some awesome blog posts and whitepapers out there that support our findings in one way or another. Writeups by When TRITON was discovered, we began to wonder how the TRITON actor reverse engineered TriStation and implemented it into the framework. We have a lot of theories, all of which seemed plausible: Did they build, buy, borrow, or steal? Or some combination thereof? Our initial theory was that the threat actor purchased a Triconex controller and software for their own testing and reverse engineering from the \"ground up\", although if this was the case we do not believe they had a controller with the exact vulnerable firmware version, else they would have had fewer problems with TRITON in practice at the victim site. They may have bought or used a demo version of the TriStation 1131 software, allowing them to reverse engineer enough of TriStation for the framework. They may have stolen TriStation Python libraries from ICS companies, subsidiaries or system integrators and used the stolen material as a base for TriStation and TRITON development. But then again, it is possible that they borrowed TriStation software, Triconex hardware and Python connectors from government-owned utility that was using them legitimately. Looking at the raw TRITON code, some of the comments may appear oddly phrased, but we do get a sense that the developer is clearly using many of the right vernacular and acronyms, showing smarts on PLC programming. The TS_cnames.pyc script contains interesting typos such as 'Set lable', 'Alocate network accepted', 'Symbol table ccepted' and 'Set program information reponse'. These appear to be normal human error and reflect neither poor written English nor laziness in coding. The significant amount of annotation, cascading logic, and robust error handling throughout the code suggests thoughtful development and testing of the framework. This complicates the theory of \"ground up\" development, so did they base their code on something else? While learning from the TriStation functionality within TRITON, we continued to explore legitimate TriStation software. We began our search for \"TS1131.exe\" and hit dead ends sorting through TriStation DLLs until we came across a variety of TriStation utilities in MSI form. We ultimately stumbled across a juicy archive containing \"Trilog v4.\" Upon further inspection, this file installed \"TriLog.exe,\" which the original TRITON executable mimicked, and a couple of supporting DLLs, all of which were timestamped around August 2006. When we saw the DLL file description \"Tricon Communications Interface\" and original file name \"TricCom.DLL\", we knew we were in the right place. With a simple look at the file strings, \"BAZINGA!\" We struck gold. 069247DF527A96A0E048732CA57E7D3D   110592   2006-08-23   Tricon Communications Interface   TricCom Dynamic Link Library   4.2.441   TricCom.DLL   Copyright © 1993-2006 Triconex Corporation     The tr1com40.DLL is exactly what you would expect to see in a custom application package. It is a library that helps support the communications for a Triconex controller. If you've pored over TRITON as much as we have, the moment you look at strings you can see the obvious overlaps between the legitimate DLL and TRITON's own TS_cnames.pyc. Each of the execution command \"error codes\" from TS_cnames.pyc are in the strings of tr1com40.DLL (Figure 8). We see \"An MP has re-educated\" and \"Invalid Tristation I command\". Even misspelled command strings verbatim such as \"Non-existant data item\" and \"Alocate network accepted\". We also see many of the same unknown values. What is obvious from this discovery is that some of the strings in TRITON are likely based on code used in communications libraries for Trident and Tricon controllers. In our brief survey of the legitimate Triconex Corporation binaries, we observed a few samples with related string tables. 2004/11/19  $Workfile:   LAGSTRS.CPP  $ $Modtime:   Jul 21 1999 17:17:26  $ $Revision:   1.0   2006/08/23  $Workfile:   TR1STRS.CPP  $ $Modtime:   May 16 2006 09:55:20  $ $Revision:   1.4   2008/07/23  $Workfile:   LAGSTRS.CPP  $ $Modtime:   Jul 21 1999 17:17:26  $ $Revision:   1.0   2008/07/23  $Workfile:   TR1STRS.CPP  $ $Modtime:   May 16 2006 09:55:20  $ $Revision:   1.4   2010/09/29  $Workfile:   LAGSTRS.CPP  $ $Modtime:   Jul 21 1999 17:17:26  $ $Revision:   1.0    2011/04/27  $Workfile:   TR1STRS.CPP  $ $Modtime:   May 16 2006 09:55:20  $ $Revision:   1.4   2011/04/27  $Workfile:   LAGSTRS.CPP  $ $Modtime:   Jul 21 1999 17:17:26  $ $Revision:   1.0   2011/04/27  $Workfile:   TR1STRS.CPP  $ $Modtime:   May 16 2006 09:55:20  $ $Revision:   1.4     We extracted the CPP string tables in TR1STRS and LAGSTRS and the TS_cnames.pyc TS_names array from TRITON, and compared the 210, 204, and 212 relevant strings from each respective file. TS_cnames.pyc TS_names and tr1com40.dll share 202 of 220 combined table strings. The remaining strings are unique to each, as seen here: Go to DOWNLOAD mode  200   Not set  209   Unk75  Bad message from module   Unk76  Bad message type   Unk77  Bad TMI version number   Unk78  Module did not respond   Unk79  Open Connection: Invalid SAP %d   Unk81  Unsupported message for this TMI version   Unk83      Wrong command        TS_cnames.pyc TS_names and Tridcom.dll (1999 CPP) shared only 151 of 268 combined table strings, showing a much smaller overlap with the seemingly older CPP library. This makes sense based on the context that Tridcom.dll is meant for a Trident controller, not a Tricon controller. It does seem as though Tr1com40.dll and TR1STRS.CPP code was based on older work. We are not shocked to find that the threat actor reversed legitimate code to bolster development of the TRITON framework. They want to work smarter, not harder, too. But after reverse engineering legitimate software and implementing the basics of the TriStation, the threat actors still had an incomplete understanding of the protocol. In TRITON's TS_cnames.pyc we saw \"Unk75\", \"Unk76\", \"Unk83\" and other values that were not present in the tr1com40.DLL strings, indicating that the TRITON threat actor may have explored the protocol and annotated their findings beyond what they reverse engineered from the DLL. The gaps in TriStation implementation show us why the actors encountered problems interacting with the Triconex controllers when using TRITON in the wild. You can see more of the Trilog and Triconex DLL files on VirusTotal. Item Name  MD5  Description   Tr1com40.dll  069247df527a96a0e048732ca57e7d3d  Tricom Communcations DLL   Data1.cab  e6a3c93a6d433cbaf6f573b6c09d76c4  Parent of Tr1com40.dll   Trilog v4.1.360R  13a3b83ba2c4236ca59aba679941c8a5  RAR Archive of TriLog   TridCom.dll  5c2ed617fdec4779cb33c89082a43100  Trident Communications DLL     Seeing Triconex systems targeted with malicious intent was new to the world six months ago. Moving forward it would be reasonable to anticipate additional frameworks, such as TRITON, designed for usage against other SIS controllers and associated technologies. If Triconex was within scope, we may see similar attacker methodologies affecting the dominant industrial safety technologies. Basic security measures do little to thwart truly persistent threat actors and monitoring only IT networks is not an ideal situation. Visibility into both the IT and OT environments is critical for detecting the various stages of an ICS intrusion. Simple detection concepts such as baseline deviation can provide insight into abnormal activity. While the TRITON framework was actively in use, how many traditional ICS “alarms” were set off while the actors tested their exploits and backdoors on the Triconex controller? How many times did the TriStation protocol, as implemented in their Python scripts, fail or cause errors because of non-standard traffic? How many TriStation UDP pings were sent and how many Connection Requests? How did these statistics compare to the baseline for TriStation traffic? There are no answers to these questions for now. We believe that we can identify these anomalies in the long run if we strive for increased visibility into ICS technologies. We hope that by holding public discussions about ICS technologies, the Infosec community can cultivate closer relationships with ICS vendors and give the world better insight into how attackers move from the IT to the OT space. We want to foster more conversations like this and generally share good techniques for finding evil. Since most of all ICS attacks involve standard IT intrusions, we should probably come together to invent and improve any guidelines for how to monitor PCs and engineering workstations that bridge the IT and OT networks. We envision a world where attacking or disrupting ICS operations costs the threat actor their cover, their toolkits, their time, and their freedom. It's an ideal world, but something nice to shoot for. There is still much to do for TRITON and TriStation. There are many more sub-message types and nuances for parsing out the nitty gritty details, which is hard to do without a controller of our own. And although we’ve published much of what we learned about the TriStation here on the blog, our work will continue as we continue our study of the protocol. Thanks to everyone who did so much public research on TRITON and TriStation. We have cited a few individuals in this blog post, but there is a lot more community-sourced information that gave us clues and leads for our research and testing of the framework and protocol. We also have to acknowledge the research performed by the TRITON attackers. We borrowed a lot of your knowledge about TriStation from the TRITON framework itself. Finally, remember that we're here to collaborate. We think most of our research is right, but if you notice any errors or omissions, or have ideas for improvements, please contact: smiller@fireeye.com. The following table consists of hex values at offset 0 in the TriStation UDP packets and the associated dictionary definitions, extracted verbatim from the TRITON framework in library TS_cnames.pyc. 1  Connection Request   2  Connection Response   3  Disconnect Request   4  Disconnect Response   5  Execution Command   6  Ping Command   7  Connection Limit Reached   8  Not Connected   9  MPS Are Dead   10  Access Denied   11  Connection Failed     The following table consists of hex values at offset 6 in the TriStation UDP packets and the associated dictionary definitions, extracted verbatim from the TRITON framework in library TS_cnames.pyc. 0  0: 'Start download all',   1  1: 'Start download change',   2  2: 'Update configuration',   3  3: 'Upload configuration',   4  4: 'Set I/O addresses',   5  5: 'Allocate network',   6  6: 'Load vector table',   7  7: 'Set calendar',   8  8: 'Get calendar',   9  9: 'Set scan time',   A  10: 'End download all',   B  11: 'End download change',   C  12: 'Cancel download change',   D  13: 'Attach TRICON',   E  14: 'Set I/O address limits',   F  15: 'Configure module',   10  16: 'Set multiple point values',   11  17: 'Enable all points',   12  18: 'Upload vector table',   13  19: 'Get CP status ',   14  20: 'Run program',   15  21: 'Halt program',   16  22: 'Pause program',   17  23: 'Do single scan',   18  24: 'Get chassis status',   19  25: 'Get minimum scan time',   1A  26: 'Set node number',   1B  27: 'Set I/O point values',   1C  28: 'Get I/O point values',   1D  29: 'Get MP status',   1E  30: 'Set retentive values',   1F  31: 'Adjust clock calendar',   20  32: 'Clear module alarms',   21  33: 'Get event log',   22  34: 'Set SOE block',   23  35: 'Record event log',   24  36: 'Get SOE data',   25  37: 'Enable OVD',   26  38: 'Disable OVD',   27  39: 'Enable all OVDs',   28  40: 'Disable all OVDs',   29  41: 'Process MODBUS',   2A  42: 'Upload network',   2B  43: 'Set lable',   2C  44: 'Configure system variables',   2D  45: 'Deconfigure module',   2E  46: 'Get system variables',   2F  47: 'Get module types',   30  48: 'Begin conversion table download',   31  49: 'Continue conversion table download',   32  50: 'End conversion table download',   33  51: 'Get conversion table',   34  52: 'Set ICM status',   35  53: 'Broadcast SOE data available',   36  54: 'Get module versions',   37  55: 'Allocate program',   38  56: 'Allocate function',   39  57: 'Clear retentives',   3A  58: 'Set initial values',   3B  59: 'Start TS2 program download',   3C  60: 'Set TS2 data area',   3D  61: 'Get TS2 data',   3E  62: 'Set TS2 data',   3F  63: 'Set program information',   40  64: 'Get program information',   41  65: 'Upload program',   42  66: 'Upload function',   43  67: 'Get point groups',   44  68: 'Allocate symbol table',   45  69: 'Get I/O address',   46  70: 'Resend I/O address',   47  71: 'Get program timing',   48  72: 'Allocate multiple functions',   49  73: 'Get node number',   4A  74: 'Get symbol table',   4B  75: 'Unk75',   4C  76: 'Unk76',   4D  77: 'Unk77',   4E  78: 'Unk78',   4F  79: 'Unk79',   50  80: 'Go to DOWNLOAD mode',   51  81: 'Unk81',   52  53  83: 'Unk83',   54  55  56  57  58  59  5A  5B  5C  5D  5E  5F  60  61  62  63  64  100: 'Command rejected',   65  101: 'Download all permitted',   66  102: 'Download change permitted',   67  103: 'Modification accepted',   68  104: 'Download cancelled',   69  105: 'Program accepted',   6A  106: 'TRICON attached',   6B  107: 'I/O addresses set',   6C  108: 'Get CP status response',   6D  109: 'Program is running',   6E  110: 'Program is halted',   6F  111: 'Program is paused',   70  112: 'End of single scan',   71  113: 'Get chassis configuration response',   72  114: 'Scan period modified',   73  115: '115',   74  116: '116',   75  117: 'Module configured',   76  118: '118',   77  119: 'Get chassis status response',   78  120: 'Vectors response',   79  121: 'Get I/O point values response',   7A  122: 'Calendar changed',   7B  123: 'Configuration updated',   7C  124: 'Get minimum scan time response',   7D  125: '125',   7E  126: 'Node number set',   7F  127: 'Get MP status response',   80  128: 'Retentive values set',   81  129: 'SOE block set',   82  130: 'Module alarms cleared',   83  131: 'Get event log response',   84  132: 'Symbol table ccepted',   85  133: 'OVD enable accepted',   86  134: 'OVD disable accepted',   87  135: 'Record event log response',   88  136: 'Upload network response',   89  137: 'Get SOE data response',   8A  138: 'Alocate network accepted',   8B  139: 'Load vector table accepted',   8C  140: 'Get calendar response',   8D  141: 'Label set',   8E  142: 'Get module types response',   8F  143: 'System variables configured',   90  144: 'Module deconfigured',   91  145: '145',   92  146: '146',   93  147: 'Get conversion table response',   94  148: 'ICM print data sent',   95  149: 'Set ICM status response',   96  150: 'Get system variables response',   97  151: 'Get module versions response',   98  152: 'Process MODBUS response',   99  153: 'Allocate program response',   9A  154: 'Allocate function response',   9B  155: 'Clear retentives response',   9C  156: 'Set initial values response',   9D  157: 'Set TS2 data area response',   9E  158: 'Get TS2 data response',   9F  159: 'Set TS2 data response',   A0  160: 'Set program information reponse',   A1  161: 'Get program information response',   A2  162: 'Upload program response',   A3  163: 'Upload function response',   A4  164: 'Get point groups response',   A5  165: 'Allocate symbol table response',   A6  166: 'Program timing response',   A7  167: 'Disable points full',   A8  168: 'Allocate multiple functions response',   A9  169: 'Get node number response',   AA  170: 'Symbol table response',   AB  AC  AD  AE  AF  B0  B1  B2  B3  B4  B5  B6  B7  B8  B9  BA  BB  BC  BD  BE  BF  C0  C1  C2  C3  C4  C5  C6  C7  C8  200: 'Wrong command',   C9  201: 'Load is in progress',   CA  202: 'Bad clock calendar data',   CB  203: 'Control program not halted',   CC  204: 'Control program checksum error',   CD  205: 'No memory available',   CE  206: 'Control program not valid',   CF  207: 'Not loading a control program',   D0  208: 'Network is out of range',   D1  209: 'Not enough arguments',   D2  210: 'A Network is missing',   D3  211: 'The download time mismatches',   D4  212: 'Key setting prohibits this operation',   D5  213: 'Bad control program version',   D6  214: 'Command not in correct sequence',   D7  215: '215',   D8  216: 'Bad Index for a module',   D9  217: 'Module address is invalid',   DA  218: '218',   DB  219: '219',   DC  220: 'Bad offset for an I/O point',   DD  221: 'Invalid point type',   DE  222: 'Invalid Point Location',   DF  223: 'Program name is invalid',   E0  224: '224',   E1  225: '225',   E2  226: '226',   E3  227: 'Invalid module type',   E4  228: '228',   E5  229: 'Invalid table type',   E6  230: '230',   E7  231: 'Invalid network continuation',   E8  232: 'Invalid scan time',   E9  233: 'Load is busy',   EA  234: 'An MP has re-educated',   EB  235: 'Invalid chassis or slot',   EC  236: 'Invalid SOE number',   ED  237: 'Invalid SOE type',   EE  238: 'Invalid SOE state',   EF  239: 'The variable is write protected',   F0  240: 'Node number mismatch',   F1  241: 'Command not allowed',   F2  242: 'Invalid sequence number',   F3  243: 'Time change on non-master TRICON',   F4  244: 'No free Tristation ports',   F5  245: 'Invalid Tristation I command',   F6  246: 'Invalid TriStation 1131 command',   F7  247: 'Only one chassis allowed',   F8  248: 'Bad variable address',   F9  249: 'Response overflow',   FA  250: 'Invalid bus',   FB  251: 'Disable is not allowed',   FC  252: 'Invalid length',   FD  253: 'Point cannot be disabled',   FE  254: 'Too many retentive variables',   FF  255: 'LOADER_CONNECT',   256: 'Unknown reject code'      