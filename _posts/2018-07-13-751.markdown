---
layout: post
title: Spark Hot Potato: Passing DataFrames Between Scala Spark and PySpark
date: 2018-07-13 01:02:15
tourl: https://www.crowdstrike.com/blog/spark-hot-potato-passing-dataframes-between-scala-spark-and-pyspark/
tags: [Crowdstrike,Source,Exploitation]
---
This blog introduces some of the innovative techniques the CrowdStrike Data Science team is using to address the unique challenges inherent in supporting a solution as robust and comprehensive as the CrowdStrike FalconŽ platform. We plan to offer more blogs like this in the future.CrowdStrikeŽ is at the forefront of Big Data technology, generating over PySpark is an incredibly useful wrapper built around the Spark framework that allows for very quick and easy development of parallelized data processing code. With the advent of DataFrames in Spark 1.6, this type of development has become even easier. However, due to performance considerations with serialization overhead when using PySpark instead of Scala Spark, there are situations in which it is more performant to use Scala code to directly interact with a DataFrame in the JVM. In this blog, we will explore the process by which one can easily leverage Scala code for performing tasks that may otherwise incur too much overhead in PySpark. For this exercise, we are employing the ever-popular First, we must create the Scala code, which we will call from inside our PySpark job. The class has been named PythonHelper.scala and it contains two methods: This will, by default, place our jar in a directory named target/scala_2.11/.Full Scala source:Now that we have some Scala methods to call from PySpark, we can write a simple Python job that will call our Scala methods. This job, named Full Python source:See how PySpark interacts with the JVM Since we are trying to avoid Python overhead, we want to take a different approach than pulling the entire DataFrame into an RDD, which is shown here:Instead, we will use a UDF to operate on the columns we are interested in and then add a column to the existing DataFrame with the results of this calculation. We define our function total_length(), which performs a simple calculation over two columns in the existing DataFrame.We also define an alias called func, which declares our function as a UDF and that it returns a float value. We are then able to use the withColumn() function on our DataFrame, and pass in our UDF to perform the calculation over the two columns. We can run the job using spark-submit like the following:Here is the output of We now have a Python DataFrame that contains an additional computed column, which we can use for further processing.In order to demonstrate calling Scala functions on an existing Python DataFrame, we will now utilize our second Scala function, Here is the output of We have successfully leveraged Scala methods from PySpark and in doing so have shown mechanisms to improve overall processing time. This strategy is especially useful in cases where one or more processing steps require heavier computation, access to JVM-based libraries, custom types or formats, etc., but which ultimately result in some modification that can be applied to a DataFrame. Since Learn more about Update: While this blog post originally covered the Office 365 Activities API, that functionality has beentttttttttttttttttttttIntroduction This analysis provides an in-depth view of the Samsam ransomware, which is developed and operatedtttttttttttttttttttttIntroduction This blog tells the story of a failed Samba exploitation attempt. The goal was totttttttttttttttt